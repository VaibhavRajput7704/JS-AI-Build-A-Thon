import { type OpenAIClientOptions as AzureOpenAIClientOptions, AzureExtensionsOptions, ChatRequestMessage, ChatCompletions, EventStream, ChatCompletionsToolDefinition, ChatCompletionsNamedToolSelection, ChatCompletionsResponseFormat } from "@azure/openai";
import { BaseChatModel, BaseChatModelParams } from "@langchain/core/language_models/chat_models";
import { BaseFunctionCallOptions, TokenUsage } from "@langchain/core/language_models/base";
import { CallbackManagerForLLMRun } from "@langchain/core/callbacks/manager";
import { BaseMessage } from "@langchain/core/messages";
import { ChatGenerationChunk, ChatResult } from "@langchain/core/outputs";
import { AzureOpenAIInput, OpenAIChatInput, OpenAIChatCallOptions } from "./types.js";
interface OpenAILLMOutput {
    tokenUsage: TokenUsage;
}
export declare function messageToOpenAIRole(message: BaseMessage): string;
export interface ChatOpenAICallOptions extends OpenAIChatCallOptions, BaseFunctionCallOptions {
    tools?: ChatCompletionsToolDefinition[];
    tool_choice?: ChatCompletionsNamedToolSelection;
    response_format?: ChatCompletionsResponseFormat;
    seed?: number;
}
/** @deprecated Import from "@langchain/openai" instead. */
export declare class AzureChatOpenAI extends BaseChatModel<ChatOpenAICallOptions> implements OpenAIChatInput, AzureOpenAIInput {
    static lc_name(): string;
    get callKeys(): string[];
    get lc_secrets(): {
        [key: string]: string;
    } | undefined;
    get lc_aliases(): Record<string, string>;
    lc_serializable: boolean;
    azureExtensionOptions?: AzureExtensionsOptions | undefined;
    maxTokens?: number | undefined;
    temperature: number;
    topP: number;
    logitBias?: Record<string, number> | undefined;
    user?: string | undefined;
    n: number;
    presencePenalty: number;
    frequencyPenalty: number;
    stop?: string[] | undefined;
    stopSequences?: string[] | undefined;
    streaming: boolean;
    modelName: string;
    model: string;
    modelKwargs?: OpenAIChatInput["modelKwargs"];
    timeout?: number | undefined;
    azureOpenAIEndpoint?: string;
    azureOpenAIApiKey?: string;
    apiKey?: string;
    azureOpenAIApiDeploymentName?: string;
    private client;
    constructor(fields?: Partial<OpenAIChatInput> & Partial<AzureOpenAIInput> & BaseChatModelParams & {
        configuration?: AzureOpenAIClientOptions;
    });
    private formatMessages;
    protected _streamChatCompletionsWithRetry(azureOpenAIMessages: ChatRequestMessage[], options: this["ParsedCallOptions"]): Promise<EventStream<ChatCompletions>>;
    _streamResponseChunks(messages: BaseMessage[], options: this["ParsedCallOptions"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<ChatGenerationChunk>;
    _generate(messages: BaseMessage[], options: this["ParsedCallOptions"], runManager?: CallbackManagerForLLMRun): Promise<ChatResult>;
    /**
     * Estimate the number of tokens an array of generations have used.
     */
    private getNumTokensFromGenerations;
    _llmType(): string;
    /**
     * Estimate the number of tokens a prompt will use.
     * Modified from: https://github.com/hmarr/openai-chat-tokens/blob/main/src/index.ts
     */
    private getEstimatedTokenCountFromPrompt;
    getNumTokensFromMessages(messages: BaseMessage[]): Promise<{
        totalCount: number;
        countPerMessage: number[];
    }>;
    /** @ignore */
    _combineLLMOutput(...llmOutputs: OpenAILLMOutput[]): OpenAILLMOutput;
}
export {};
